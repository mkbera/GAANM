{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "TEXT = data.Field(tokenize = 'spacy', init_token = '<sos>', eos_token = '<eos>', lower = True)\n",
    "LABEL = data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 25000\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_data, valid_data = train_data.split(random_state = random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 17500\n",
      "Number of validation examples: 7500\n",
      "Number of testing examples: 25000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25_000\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE)\n",
    "\n",
    "# TEXT.build_vocab(train_data, \n",
    "#                  max_size = MAX_VOCAB_SIZE, \n",
    "#                  vectors = \"glove.6B.100d\", \n",
    "#                  unk_init = torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['once', 'big', 'action', 'star', 'who', 'fell', 'off', 'the', 'face', 'of', 'the', 'earth', 'ends', 'up', 'in', 'a', 'small', 'town', 'with', 'a', 'problem', 'with', 'drug', 'dealers', 'and', 'a', 'dead', 'body', 'of', 'a', 'federal', 'agent']\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_data.examples[0])['text'][:32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in TEXT vocabulary: 25004\n",
      "Unique tokens in LABEL vocabulary: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '<sos>', '<eos>', 'the', ',', '.', 'and', 'a', 'of']\n"
     ]
    }
   ],
   "source": [
    "# print(TEXT.vocab.freqs.most_common(20))\n",
    "print(TEXT.vocab.itos[:10])\n",
    "# print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNpart1(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        return hidden.squeeze(0) # [batch_size, hid dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNpart2(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, hidden_squeeze):\n",
    "        return self.fc(hidden_squeeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data), \n",
    "    batch_size = BATCH_SIZE,\n",
    "    device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "encoder = RNNpart1(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, PAD_IDX)\n",
    "classifer = RNNpart2(HIDDEN_DIM, OUTPUT_DIM)\n",
    "# print(encoder)\n",
    "# print(classifer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 2,592,048 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad) \n",
    "\n",
    "# print(f'The model has {count_parameters(encoder):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "print(pretrained_embeddings.shape)\n",
    "encoder.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import itertools\n",
    "# params = [model1.parameters(), model2.parameters()]\n",
    "params = itertools.chain(encoder.parameters(), classifer.parameters())\n",
    "\n",
    "optimizer = optim.SGD(params, lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "classifer = classifer.to(device)\n",
    "\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder, classifer, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    encoder.train()\n",
    "    classifer.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        hidden_vec = encoder(batch.text) \n",
    "        predictions = classifer(hidden_vec).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, classifer, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "#             predictions = model2(model1(batch.text)).squeeze(1)\n",
    "            hidden_vec = encoder(batch.text) \n",
    "            predictions = classifer(hidden_vec).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 9m 9s\n",
      "\tTrain Loss: 0.693 | Train Acc: 50.52%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 50.73%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(encoder, classifer, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(encoder, classifer, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(encoder.state_dict(), 'encoder.pt')\n",
    "        torch.save(classifer.state_dict(), 'classifer.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.687 | Test Acc: 56.23%\n"
     ]
    }
   ],
   "source": [
    "# encoder = RNNpart1(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, PAD_IDX)\n",
    "# classifer = RNNpart2(HIDDEN_DIM, OUTPUT_DIM)\n",
    "encoder.load_state_dict(torch.load('encoder.pt'))\n",
    "classifer.load_state_dict(torch.load('classifer.pt'))\n",
    "\n",
    "encoder.to(device)\n",
    "classifer.to(device)\n",
    "\n",
    "test_loss, test_acc = evaluate(model1, model2, test_iterator, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNdecoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_dim, embedding_dim, vocab_size, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, hidden, text):\n",
    "        #text = [batch size]\n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        #hidden = [n layers * n directions = 1, batch size, hid dim]\n",
    "        inp = text.unsqueeze(0)\n",
    "        #inp = [1, batch size]\n",
    "        embedded = self.embedding(inp)\n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        output, hidden = self.rnn(embedded,hidden)\n",
    "        #output = [seq len=1, batch size, hid dim * (n directions=1)]\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        #prediction = [batch size, vocab size]\n",
    "        hidden = hidden.squeeze(0)\n",
    "        #hidden = [batch size, hid dim]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "encoder = RNNpart1(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, PAD_IDX)\n",
    "decoder = RNNdecoder(HIDDEN_DIM, EMBEDDING_DIM, VOCAB_SIZE, PAD_IDX)\n",
    "# print(encoder)\n",
    "# print(classifer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(torch.load('encoder.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "decoder.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import itertools\n",
    "\n",
    "optimizer = optim.SGD(decoder.parameters(), lr=1e-3)\n",
    "\n",
    "TEXT_PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "x_loss = nn.CrossEntropyLoss(ignore_index = TEXT_PAD_IDX)\n",
    "z_loss = nn.MSELoss(reduction='mean')\n",
    "x_wt = random.random()\n",
    "\n",
    "encoder = encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decoder(encoder, decoder , iterator, optimizer, device, x_loss, z_loss, x_wt, clip):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    decoder.train()\n",
    "    \n",
    "    for i,batch in enumerate(iterator):\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text = batch.text\n",
    "        #text = [sent len, batch size]\n",
    "        sent_len, batch_size = text.shape\n",
    "        \n",
    "        hidden_vec = encoder(text) \n",
    "        # hidden_vec = [batch_size, hid dim]\n",
    "        \n",
    "        trg_len = min(sent_len, 32)\n",
    "\n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, VOCAB_SIZE).to(device)   \n",
    "        pred_text = torch.zeros(trg_len, batch_size, dtype=torch.long).to(device) \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        wrd = torch.LongTensor([ TEXT.vocab.stoi[TEXT.init_token] ]*batch_size)\n",
    "        pred_text[0] = TEXT.vocab.stoi[TEXT.init_token]\n",
    "        hidden = hidden_vec\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "\n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden = decoder(hidden, wrd)\n",
    "            #output = [batch size, vocab size]\n",
    "            #hidden = [batch_size, hid dim]\n",
    "        \n",
    "\n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "\n",
    "            #decide if we are going to use teacher forcing ratio = 0.5\n",
    "            teacher_force = random.random() < 0.5\n",
    "\n",
    "            #get the highest predicted token from our predictions\n",
    "            top_wrd = output.argmax(1) \n",
    "            pred_text[t] = top_wrd\n",
    "\n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            wrd = text[t] if teacher_force else top_wrd\n",
    "        \n",
    "        hidden_vec1 = encoder(pred_text)\n",
    "        #text = [sent len, batch size]\n",
    "        #pred_text = [trg_len = min(sent_len, 32), batch size]\n",
    "\n",
    "        outputs = outputs[1:].view(-1, VOCAB_SIZE)\n",
    "        text1 = text[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "#         print(trg.shape,output.shape)\n",
    "        trg_len = outputs.shape[0]\n",
    "        trg = text1[:trg_len]\n",
    "        \n",
    "        loss = x_wt*x_loss(outputs, trg) + (1-x_wt)*z_loss(hidden_vec, hidden_vec1)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "      \n",
    "\n",
    "        if i%50 == 0 :\n",
    "            print('batch_no :',i)\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_decoder(encoder, decoder , iterator, device, x_loss, z_loss, x_wt):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    decoder.eval()\n",
    "    \n",
    "    for i,batch in enumerate(iterator):\n",
    "        \n",
    "        text = batch.text\n",
    "        #text = [sent len, batch size]\n",
    "        sent_len, batch_size = text.shape\n",
    "        \n",
    "        hidden_vec = encoder(text) \n",
    "        # hidden_vec = [batch_size, hid dim]\n",
    "        \n",
    "        trg_len = 32\n",
    "\n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, VOCAB_SIZE).to(device)   \n",
    "        pred_text = torch.zeros(trg_len, batch_size, dtype=torch.long).to(device) \n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        wrd = torch.LongTensor([ TEXT.vocab.stoi[TEXT.init_token] ]*batch_size)\n",
    "        pred_text[0] = TEXT.vocab.stoi[TEXT.init_token]\n",
    "        hidden = hidden_vec\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "\n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden = decoder(hidden, wrd)\n",
    "            #output = [batch size, vocab size]\n",
    "            #hidden = [batch_size, hid dim]\n",
    "        \n",
    "\n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "\n",
    "            #get the highest predicted token from our predictions\n",
    "            top_wrd = output.argmax(1) \n",
    "            pred_text[t] = top_wrd\n",
    "\n",
    "  \n",
    "        hidden_vec1 = encoder(pred_text)\n",
    "        #text = [sent len, batch size]\n",
    "        #pred_text = [trg_len = min(sent_len, 32), batch size]\n",
    "\n",
    "        outputs = outputs[1:].view(-1, VOCAB_SIZE)\n",
    "        text1 = text[1:].view(-1)\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "#         print(trg.shape,output.shape)\n",
    "        trg_len = outputs.shape[0]\n",
    "        trg = text1[:trg_len]\n",
    "        \n",
    "        loss = x_wt*x_loss(outputs, trg) + (1-x_wt)*z_loss(hidden_vec, hidden_vec1)\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "      \n",
    "\n",
    "        if i%50 == 0 :\n",
    "            print('batch_no :',i)\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_no : 0\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1\n",
    "CLIP = 1\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train_decoder(encoder, decoder, train_iterator, optimizer, device, x_loss, z_loss, x_wt, CLIP)\n",
    "    valid_loss = evaluate_decoder(encoder, decoder, valid_iterator, device, x_loss, z_loss, x_wt)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(decoder_model.state_dict(), 'decoder.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tr, valid_iterator2 = data.BucketIterator.splits(\n",
    "    (train_data, valid_data), \n",
    "    batch_size = 1,\n",
    "    device = device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FGSM:\n",
    "    \"\"\"\n",
    "    Fast Gradient Sign Method\n",
    "    Ian J. Goodfellow, Jonathon Shlens, Christian Szegedy.\n",
    "    Explaining and Harnessing Adversarial Examples.\n",
    "    ICLR, 2015\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=0.15, clip_max=0.5, clip_min=-0.5):\n",
    "#         super(FGSM, self).__init__(clip_max, clip_min)\n",
    "        self.clip_max = clip_max\n",
    "        self.clip_min = clip_min\n",
    "        self.eps = eps\n",
    "        self.fun = self.Function\n",
    "    def Function (self, x):\n",
    "        \n",
    "        \n",
    "    def generate(self, model, criterion, x, y):\n",
    "        model.eval()\n",
    "        nx = x.clone().detach()\n",
    "        ny = y\n",
    "#         nx = torch.unsqueeze(x, 0)\n",
    "#         ny = torch.unsqueeze(y, 0)\n",
    "        nx.requires_grad_()\n",
    "        out = model(nx).squeeze(1)\n",
    "#         loss = F.cross_entropy(out, ny)\n",
    "        loss = criterion(out, ny)\n",
    "        loss.backward()\n",
    "        x_adv = nx + self.eps * torch.sign(nx.grad.data)\n",
    "        x_adv.clamp_(self.clip_min, self.clip_max)\n",
    "#         x_adv.squeeze_(0)\n",
    "        return x_adv.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BIM:\n",
    "    \"\"\"\n",
    "    Basic Iterative Method\n",
    "    Alexey Kurakin, Ian J. Goodfellow, Samy Bengio.\n",
    "    Adversarial Examples in the Physical World.\n",
    "    arXiv, 2016\n",
    "    \"\"\"\n",
    "    def __init__(self, eps=0.15, eps_iter=0.01, n_iter=50, clip_max=0.5, clip_min=-0.5):\n",
    "        self.clip_max = clip_max\n",
    "        self.clip_min = clip_min\n",
    "#         super(BIM, self).__init__(clip_max, clip_min)\n",
    "        self.eps = eps\n",
    "        self.eps_iter = eps_iter\n",
    "        self.n_iter = n_iter\n",
    "#         self.device = device\n",
    "\n",
    "    def generate(self, model, criterion, x, y):\n",
    "        model.eval()\n",
    "#         x.to(self.device)\n",
    "        nx = x.clone().detach()\n",
    "        ny = y\n",
    "#         nx = torch.unsqueeze(x, 0)\n",
    "#         ny = torch.unsqueeze(y, 0)\n",
    "        nx.requires_grad_()\n",
    "        eta = torch.zeros(nx.shape)\n",
    "        if torch.cuda.is_available():\n",
    "            eta = eta.cuda()\n",
    "        for i in range(self.n_iter):\n",
    "            out = model(nx+eta).squeeze(1)\n",
    "#             loss = F.cross_entropy(out, ny)\n",
    "            loss = criterion(out, ny)\n",
    "            loss.backward()\n",
    "            eta += self.eps_iter * torch.sign(nx.grad.data)\n",
    "            eta.clamp_(-self.eps, self.eps)\n",
    "            nx.grad.data.zero_()\n",
    "        x_adv = nx + eta\n",
    "        x_adv.clamp_(self.clip_min, self.clip_max)\n",
    "#         x_adv.squeeze_(0)\n",
    "        return x_adv.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFool():\n",
    "    \"\"\"\n",
    "    DeepFool\n",
    "    Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Pascal Frossard\n",
    "    DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks.\n",
    "    CVPR, 2016\n",
    "    \"\"\"\n",
    "    def __init__(self, max_iter=50, clip_max=0.5, clip_min=-0.5):\n",
    "#         super(DeepFool, self).__init__(clip_max, clip_min)\n",
    "        self.clip_max = clip_max\n",
    "        self.clip_min = clip_min\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def generate(self, model, criterion, x, y):\n",
    "        model.eval()\n",
    "        nx = x.clone().detach()\n",
    "        nx.requires_grad_()\n",
    "        ny = y\n",
    "#         eta = torch.zeros(nx.shape)\n",
    "#         if torch.cuda.is_available():\n",
    "#             eta = eta.cuda()\n",
    "#         eta.requires_grad_()\n",
    "#         out = model(nx+eta)\n",
    "        for i in range(self.max_iter):\n",
    "            out = model(nx).squeeze(1)\n",
    "            if torch.sign(out) == torch.sign(ny):\n",
    "                break\n",
    "            out.backward()\n",
    "            grad_np = nx.grad.data.clone()\n",
    "#             print(type(nx.grad))\n",
    "            nx.data += - (out * grad_np) / torch.norm(grad_np)\n",
    "#             nx.clamp_(self.clip_min, self.clip_max)\n",
    "#             print(type(nx.grad) )\n",
    "            nx.grad.data.zero_()\n",
    "        x_adv = nx \n",
    "        x_adv.requires_grad_(False)\n",
    "        x_adv.clamp_(self.clip_min, self.clip_max)\n",
    "#         x_adv.squeeze_(0)\n",
    "        return x_adv.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1221]], device='cuda:0') tensor([[0.0024]], device='cuda:0') tensor([0.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "attack = DeepFool()\n",
    "# attack = FGSM()\n",
    "# attack = BIM()\n",
    "for batch in valid_iterator2:\n",
    "    z = model1(batch.text)\n",
    "\n",
    "    z_clone = z.clone().detach()\n",
    "#     print(z_clone.is_cuda)\n",
    "#     break\n",
    "    z_new = attack.generate(model2, criterion, z_clone, batch.label)\n",
    "#     print(z)\n",
    "#     print(z_new)\n",
    "    y = model2(z)\n",
    "    y_new = model2(z_new)\n",
    "    print(y.data, y_new.data, batch.label)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seq",
   "language": "python",
   "name": "seq2seq"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
